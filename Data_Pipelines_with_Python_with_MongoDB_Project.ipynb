{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFzh23PHa-Tx"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import logging\n",
        "from pymongo import UpdateOne,DeleteOne\n",
        "from pymongo.errors import BulkWriteError\n",
        "\n",
        "\n",
        "# Extraction function\n",
        "def extract_data():\n",
        "    # Load call log data from CSV file\n",
        "    call_logs = pd.read_csv('call_logs.csv')\n",
        "\n",
        "    # Convert call duration to minutes for easier analysis\n",
        "    call_logs['duration_minutes'] = call_logs['call_duration'] / 60\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data extraction completed.\")\n",
        "\n",
        "    return call_logs\n",
        "\n",
        "# Transformation function\n",
        "def transform_data(call_logs):\n",
        "    # Data cleaning and handling missing values\n",
        "    transformed_data=call_logs.dropna()\n",
        "    transform_data = call_logs.drop_duplicates()\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data transformation completed.\")\n",
        "    transformed_data= transformed_data.to_dict('records')\n",
        "    return transformed_data\n",
        "\n",
        "# Loading function\n",
        "def load_data(transformed_data):\n",
        "    # Connect to MongoDB\n",
        "    client = pymongo.MongoClient(\"mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority\",ssl=True,tlsInsecure=True)\n",
        "    db = client[\"galugalu\"]\n",
        "    collection = db[\"galugalu\"]\n",
        "\n",
        "\n",
        "\n",
        "    # Create indexes on the collection and compress data using snappy algorithm\n",
        "    collection.create_index([('call_duration',pymongo.DESCENDING)],\n",
        "                           storageEngine = {\n",
        "                           'wiredTiger': {\n",
        "                              'configString': 'block_compressor=snappy'\n",
        "                           }\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    # Use bulk inserts to optimize performance\n",
        "\n",
        "    collection.insert_many(transformed_data)\n",
        "\n",
        "    #Demonstrate the ability to execute mixed bulk write operations, will be combining one update and Delete operations\n",
        "\n",
        "    requests = [\n",
        "        UpdateOne({\"call_id\":1},{'$set':{'call_type':'Incoming'}}),\n",
        "        DeleteOne({'call_id':2})\n",
        "    ]\n",
        "    try:\n",
        "        collection.bulk_write(requests)\n",
        "    except BulkWriteError as bwe:\n",
        "       pprint(bwe.details)\n",
        "\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data loading completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    data = extract_data()\n",
        "    transformed_data = transform_data(data)\n",
        "    load_data(transformed_data)"
      ]
    }
  ]
}